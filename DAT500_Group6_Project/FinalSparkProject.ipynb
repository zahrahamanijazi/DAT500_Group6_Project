{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "2023-05-05 16:28:13,393 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"spark-proj111\") \\\n",
    "    .master('yarn')\\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import regexp_replace\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import ArrayType, StringType\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.ml.feature import StopWordsRemover, HashingTF, IDF\n",
    "from pyspark.ml.clustering import KMeans, BisectingKMeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "from pyspark.ml.linalg import Vectors\n",
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.sql.functions import sum, col\n",
    "from delta.tables import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Data Schema\n",
    "schema = StructType([StructField(\"_c0\", IntegerType(), True),\n",
    "                            StructField(\"_c1\", StringType(), True),\n",
    "                            StructField(\"_c2\", IntegerType(), True),\n",
    "                            StructField(\"_c3\", IntegerType(), True),\n",
    "                            ])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# creating our  DataFrame by reading our CSV file on HDFS\n",
    "df = spark.read \\\n",
    "    .schema(schema)\\\n",
    "    .option(\"delimiter\", \"\\t\") \\\n",
    "    .csv(\"hdfs://namenode:9000/project/test18/part*\") \\\n",
    "    .select( \"_c0\", \"_c1\", \"_c3\")\\\n",
    "    .distinct()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.repartition(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|title                                                                                                                  |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "|Ontology-Mediated Queries: Combined Complexity and Succinctness of Rewritings via Circuit Complexity                   |\n",
      "|Inducing strong convergence into the asymptotic behaviour of proximal splitting algorithms in Hilbert spaces           |\n",
      "|Self-Supervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation                   |\n",
      "|Context-dependent upper-confidence bounds for directed exploration                                                     |\n",
      "|Unsupervised object-level video summarization with online motion auto-encoder                                          |\n",
      "|Evaluation of the finite element lattice Boltzmann method for binary fluid flows                                       |\n",
      "|Universality and individuality in neural dynamics across large populations of recurrent networks                       |\n",
      "|How to Split UL/DL Antennas in Full-Duplex Cellular Networks                                                           |\n",
      "|Stable Recurrent Models                                                                                                |\n",
      "|Products of Borel fixed ideals of maximal minors                                                                       |\n",
      "|Fine-grained categorization via CNN-based automatic extraction and integration of object-level and part-level features☆|\n",
      "|Tractability properties of the weighted star discrepancy of the Halton sequence                                        |\n",
      "|Sub-Linear Time Support Recovery for Compressed Sensing Using Sparse-Graph Codes                                       |\n",
      "|A Poisson Gamma Probabilistic Model for Latent Node-group Memberships in Dynamic Networks                              |\n",
      "|Neuroevolution-based Inverse Reinforcement Learning                                                                    |\n",
      "|Optimal Approximate Matrix Product in Terms of Stable Rank                                                             |\n",
      "|On the dual representation of coherent risk measures                                                                   |\n",
      "|Resource Allocation for a Wireless Coexistence Management System based on Reinforcement Learning                       |\n",
      "|The Eddy Current--LLG Equations: FEM-BEM Coupling and A Priori Error Estimates                                         |\n",
      "|Contents for a Model-Based Software Engineering Body of Knowledge                                                      |\n",
      "|On Model Predictive Path Following and Trajectory Tracking for Industrial Robots                                       |\n",
      "|Some Remarks on Quasi-Generalized CR-Null Geometry in Indefinite Nearly Cosymplectic Manifolds                         |\n",
      "|Sem-GAN: Semantically-Consistent Image-to-Image Translation                                                            |\n",
      "|Design of Symbolic Controllers for Networked Control Systems                                                           |\n",
      "|Recent developments in natural computation                                                                             |\n",
      "|Imitation-Regularized Offline Learning                                                                                 |\n",
      "|A Reliable Channel Estimation Scheme Using Scattered Pilot Pattern for IEEE 802.22-based Mobile Communication System   |\n",
      "|The Expressive Power of Higher-Order Datalog.                                                                          |\n",
      "|Residue Field Domination in Real Closed Valued Fields                                                                  |\n",
      "|FastGCN: Fast Learning with Graph Convolutional Networks via Importance Sampling                                       |\n",
      "+-----------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Rename the name of columns to be the same as dataset fields\n",
    "df = df.withColumnRenamed('_c0', 'id')\\\n",
    "    .withColumnRenamed('_c1', 'title')\\\n",
    "    .withColumnRenamed('_c2', 'year')\\\n",
    "    .withColumnRenamed('_c3', 'n_citation')\n",
    "df.select(\"title\").show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-27 07:02:47,358 ERROR netty.Inbox: An error happened while processing message in the inbox for CoarseGrainedScheduler\n",
      "java.lang.OutOfMemoryError: Java heap space\n",
      "\tat java.util.Arrays.copyOf(Arrays.java:3236)\n",
      "\tat java.io.ByteArrayOutputStream.grow(ByteArrayOutputStream.java:118)\n",
      "\tat java.io.ByteArrayOutputStream.ensureCapacity(ByteArrayOutputStream.java:93)\n",
      "\tat java.io.ByteArrayOutputStream.write(ByteArrayOutputStream.java:153)\n",
      "\tat org.apache.spark.util.ByteBufferOutputStream.write(ByteBufferOutputStream.scala:41)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.drain(ObjectOutputStream.java:1877)\n",
      "\tat java.io.ObjectOutputStream$BlockDataOutputStream.setBlockDataMode(ObjectOutputStream.java:1786)\n",
      "\tat java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1189)\n",
      "\tat java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:348)\n",
      "\tat org.apache.spark.serializer.JavaSerializationStream.writeObject(JavaSerializer.scala:44)\n",
      "\tat org.apache.spark.serializer.JavaSerializerInstance.serialize(JavaSerializer.scala:101)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.prepareLaunchingTask(TaskSetManager.scala:512)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.$anonfun$resourceOffer$2(TaskSetManager.scala:478)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager$$Lambda$1516/1144272361.apply(Unknown Source)\n",
      "\tat scala.Option.map(Option.scala:230)\n",
      "\tat org.apache.spark.scheduler.TaskSetManager.resourceOffer(TaskSetManager.scala:455)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2(TaskSchedulerImpl.scala:395)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$2$adapted(TaskSchedulerImpl.scala:390)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$1507/1999635316.apply(Unknown Source)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOfferSingleTaskSet$1(TaskSchedulerImpl.scala:390)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$1506/457301967.apply$mcVI$sp(Unknown Source)\n",
      "\tat scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:158)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.resourceOfferSingleTaskSet(TaskSchedulerImpl.scala:381)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20(TaskSchedulerImpl.scala:587)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$20$adapted(TaskSchedulerImpl.scala:582)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl$$Lambda$1505/2072292933.apply(Unknown Source)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach(IndexedSeqOptimized.scala:36)\n",
      "\tat scala.collection.IndexedSeqOptimized.foreach$(IndexedSeqOptimized.scala:33)\n",
      "\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:198)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16(TaskSchedulerImpl.scala:582)\n",
      "\tat org.apache.spark.scheduler.TaskSchedulerImpl.$anonfun$resourceOffers$16$adapted(TaskSchedulerImpl.scala:555)\n",
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 0) / 1]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ubuntu/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "  File \"/home/ubuntu/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py\", line 511, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "  File \"/usr/lib/python3.8/socket.py\", line 669, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m new_df \u001b[39m=\u001b[39m spark\u001b[39m.\u001b[39mcreateDataFrame(new_rows)\n\u001b[1;32m     23\u001b[0m \u001b[39m# Show the first 30 rows of the new DataFrame\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m new_df\u001b[39m.\u001b[39;49mshow(\u001b[39m30\u001b[39;49m, truncate\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     25\u001b[0m df\u001b[39m=\u001b[39mnew_df\n",
      "File \u001b[0;32m~/spark/python/pyspark/sql/dataframe.py:502\u001b[0m, in \u001b[0;36mDataFrame.show\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    498\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mValueError\u001b[39;00m:\n\u001b[1;32m    499\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[1;32m    500\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mParameter \u001b[39m\u001b[39m'\u001b[39m\u001b[39mtruncate=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m should be either bool or int.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(truncate))\n\u001b[0;32m--> 502\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mshowString(n, int_truncate, vertical))\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1320\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1313\u001b[0m args_command, temp_args \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_build_args(\u001b[39m*\u001b[39margs)\n\u001b[1;32m   1315\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1316\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1320\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1321\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[1;32m   1322\u001b[0m     answer, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtarget_id, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname)\n\u001b[1;32m   1324\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/java_gateway.py:1038\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1036\u001b[0m connection \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_connection()\n\u001b[1;32m   1037\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1038\u001b[0m     response \u001b[39m=\u001b[39m connection\u001b[39m.\u001b[39;49msend_command(command)\n\u001b[1;32m   1039\u001b[0m     \u001b[39mif\u001b[39;00m binary:\n\u001b[1;32m   1040\u001b[0m         \u001b[39mreturn\u001b[39;00m response, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m~/spark/python/lib/py4j-0.10.9.5-src.zip/py4j/clientserver.py:511\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 511\u001b[0m         answer \u001b[39m=\u001b[39m smart_decode(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstream\u001b[39m.\u001b[39;49mreadline()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m])\n\u001b[1;32m    512\u001b[0m         logger\u001b[39m.\u001b[39mdebug(\u001b[39m\"\u001b[39m\u001b[39mAnswer received: \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(answer))\n\u001b[1;32m    513\u001b[0m         \u001b[39m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    514\u001b[0m         \u001b[39m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.8/socket.py:669\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    667\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    668\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 669\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    670\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    671\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Remove \".\" from the end of titles\n",
    "# inefficient due to shuffle\n",
    "new_rows = []\n",
    "\n",
    "# iterate on DataFrame rows\n",
    "for row in df.rdd.collect():\n",
    "    # Get the value of the \"title\" column for each row\n",
    "    old_value = row[\"title\"]\n",
    "    \n",
    "    # Cleaning titles\n",
    "    # Removing the period at the end of titles\n",
    "    new_value = old_value.rstrip(\".\")\n",
    "    \n",
    "    # Append modified value to the new rows\n",
    "    new_row = row.asDict()\n",
    "    new_row[\"_title_cleaned\"] = new_value\n",
    "    new_rows.append(new_row)\n",
    "\n",
    "# Form a new DataFrame from the list of new rows\n",
    "new_df = spark.createDataFrame(new_rows)\n",
    "\n",
    "new_df.show(30, truncate=False)\n",
    "df=new_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n",
      "|title                                                                                                                 |_title_cleaned                                                                                                        |\n",
      "+----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n",
      "|Matrix Profile XVI: Efficient and Effective Labeling of Massive Time Series Archives                                  |Matrix Profile XVI: Efficient and Effective Labeling of Massive Time Series Archives                                  |\n",
      "|Enhancing CS1 with Mobile Apps.                                                                                       |Enhancing CS1 with Mobile Apps                                                                                        |\n",
      "|Mobile Crowdsourcing and Pervasive Computing for Smart Cities                                                         |Mobile Crowdsourcing and Pervasive Computing for Smart Cities                                                         |\n",
      "|Readability and computer documentation                                                                                |Readability and computer documentation                                                                                |\n",
      "|Space as interface: creating interactive street art                                                                   |Space as interface: creating interactive street art                                                                   |\n",
      "|Comparison of Several Difference Schemes on 1D and 2D Test Problems for the Euler Equations                           |Comparison of Several Difference Schemes on 1D and 2D Test Problems for the Euler Equations                           |\n",
      "|Foot Trajectory Planning Method with Adjustable Parameters for Complex Environment                                    |Foot Trajectory Planning Method with Adjustable Parameters for Complex Environment                                    |\n",
      "|Evolutionary optimization of sliding contact positions in powered floor systems for mobile robots                     |Evolutionary optimization of sliding contact positions in powered floor systems for mobile robots                     |\n",
      "|Multi-scale vortical structure analysis on large eddy simulation of dune wake flow                                    |Multi-scale vortical structure analysis on large eddy simulation of dune wake flow                                    |\n",
      "|A Gaussian kernel-based fuzzy c-means algorithm with a spatial bias correction                                        |A Gaussian kernel-based fuzzy c-means algorithm with a spatial bias correction                                        |\n",
      "|Towards building a universal defect prediction model with rank transformed predictors                                 |Towards building a universal defect prediction model with rank transformed predictors                                 |\n",
      "|Freedom of choice, ease of use, and the formation of interface preferences                                            |Freedom of choice, ease of use, and the formation of interface preferences                                            |\n",
      "|High Order Still-Water and Moving-Water Equilibria Preserving Discontinuous Galerkin Methods for the Ripa Model       |High Order Still-Water and Moving-Water Equilibria Preserving Discontinuous Galerkin Methods for the Ripa Model       |\n",
      "|Surfaces from contours                                                                                                |Surfaces from contours                                                                                                |\n",
      "|Program synthesis: opportunities for the next decade                                                                  |Program synthesis: opportunities for the next decade                                                                  |\n",
      "|A symbol-based intelligent control system with self-exploration process                                               |A symbol-based intelligent control system with self-exploration process                                               |\n",
      "|An hybrid intelligent approach for real-time traffic control                                                          |An hybrid intelligent approach for real-time traffic control                                                          |\n",
      "|A Discussion of User Experience on a Panoramic Scooter Riding Video Service                                           |A Discussion of User Experience on a Panoramic Scooter Riding Video Service                                           |\n",
      "|Ultrahigh Dimensional Feature Selection: Beyond The Linear Model                                                      |Ultrahigh Dimensional Feature Selection: Beyond The Linear Model                                                      |\n",
      "|In Situ Wireless Channel Visualization Using Augmented Reality and Ray Tracing                                        |In Situ Wireless Channel Visualization Using Augmented Reality and Ray Tracing                                        |\n",
      "|Optimized Layout of the Soil Moisture Sensor in Tea Plantations Based on Improved Dijkstra Algorithm.                 |Optimized Layout of the Soil Moisture Sensor in Tea Plantations Based on Improved Dijkstra Algorithm                  |\n",
      "|Semantic Round-Tripping in Conceptual Modelling Using Restricted Natural Language.                                    |Semantic Round-Tripping in Conceptual Modelling Using Restricted Natural Language                                     |\n",
      "|Results-Based Project Follow-Up: A Method for Ongoing and Completed Projects                                          |Results-Based Project Follow-Up: A Method for Ongoing and Completed Projects                                          |\n",
      "|Adversarial Autoencoders for Compact Representations of 3D Point Clouds                                               |Adversarial Autoencoders for Compact Representations of 3D Point Clouds                                               |\n",
      "|Digital and physical barriers to changing identities                                                                  |Digital and physical barriers to changing identities                                                                  |\n",
      "|An Analysis of Test Data Selection Criteria Using the RELAY Model of Fault Detection                                  |An Analysis of Test Data Selection Criteria Using the RELAY Model of Fault Detection                                  |\n",
      "|Short-term Forecasting of Electricity Consumption in Buildings for Efficient and Optimal Distributed Energy Management|Short-term Forecasting of Electricity Consumption in Buildings for Efficient and Optimal Distributed Energy Management|\n",
      "|Approximating the Bandwidth via Volume Respecting Embeddings                                                          |Approximating the Bandwidth via Volume Respecting Embeddings                                                          |\n",
      "|Particle animation and rendering using data parallel computation                                                      |Particle animation and rendering using data parallel computation                                                      |\n",
      "|Using data mining approaches to identify voice over IP spam                                                           |Using data mining approaches to identify voice over IP spam                                                           |\n",
      "+----------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 30 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Efficient approach in spark to Remove \".\" from the end of titles\n",
    "\n",
    "df = df.withColumn(\"_title_cleaned\", regexp_replace(\"title\", \"\\.$\", \"\"))\n",
    "df.select(\"title\",\"_title_cleaned\").show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|SPARK_PARTITION_ID()| count|\n",
      "+--------------------+------+\n",
      "|                   1|806368|\n",
      "|                   2|806505|\n",
      "|                   5|808522|\n",
      "|                   0|807334|\n",
      "|                   3|807989|\n",
      "|                   4|808296|\n",
      "|                   6| 49067|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as F\n",
    "df.groupBy(F.spark_partition_id()).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.withColumn('salt', F.rand())\n",
    "df=df.repartition(8,'salt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|SPARK_PARTITION_ID()| count|\n",
      "+--------------------+------+\n",
      "|                   1|611873|\n",
      "|                   3|610666|\n",
      "|                   4|612004|\n",
      "|                   6|612426|\n",
      "|                   0|611718|\n",
      "|                   2|610717|\n",
      "|                   5|613059|\n",
      "|                   7|611618|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "df.groupBy(F.spark_partition_id()).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:===========================================================(1 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "|title                                                                                                              |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "|OntologyMediated Queries Combined Complexity and Succinctness of Rewritings via Circuit Complexity                 |\n",
      "|Inducing strong convergence into the asymptotic behaviour of proximal splitting algorithms in Hilbert spaces       |\n",
      "|SelfSupervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation                |\n",
      "|Contextdependent upperconfidence bounds for directed exploration                                                   |\n",
      "|Unsupervised objectlevel video summarization with online motion autoencoder                                        |\n",
      "|Evaluation of the finite element lattice Boltzmann method for binary fluid flows                                   |\n",
      "|Universality and individuality in neural dynamics across large populations of recurrent networks                   |\n",
      "|How to Split ULDL Antennas in FullDuplex Cellular Networks                                                         |\n",
      "|Stable Recurrent Models                                                                                            |\n",
      "|Products of Borel fixed ideals of maximal minors                                                                   |\n",
      "|Finegrained categorization via CNNbased automatic extraction and integration of objectlevel and partlevel features☆|\n",
      "|Tractability properties of the weighted star discrepancy of the Halton sequence                                    |\n",
      "|SubLinear Time Support Recovery for Compressed Sensing Using SparseGraph Codes                                     |\n",
      "|A Poisson Gamma Probabilistic Model for Latent Nodegroup Memberships in Dynamic Networks                           |\n",
      "|Neuroevolutionbased Inverse Reinforcement Learning                                                                 |\n",
      "|Optimal Approximate Matrix Product in Terms of Stable Rank                                                         |\n",
      "|On the dual representation of coherent risk measures                                                               |\n",
      "|Resource Allocation for a Wireless Coexistence Management System based on Reinforcement Learning                   |\n",
      "|The Eddy CurrentLLG Equations FEMBEM Coupling and A Priori Error Estimates                                         |\n",
      "|Contents for a ModelBased Software Engineering Body of Knowledge                                                   |\n",
      "+-------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Second phase of cleaning for the titles\n",
    "# Deleting all Characters including : [!,?/:\"().;$#%&*()@^=+-]\n",
    "characters_to_remove = '[!,?/:\"().;$#%&*()@^=+-]'\n",
    "\n",
    "df = df.withColumn(\"title\", regexp_replace(\"_title_cleaned\", characters_to_remove, \"\")) \\\n",
    "       .withColumn(\"title\", regexp_replace(\"title\", \"\\d+\", \"\")) \\\n",
    "       .withColumn(\"title\", regexp_replace(\"title\", \":\", \"\"))\n",
    "\n",
    "df.select(\"title\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 0) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|SPARK_PARTITION_ID()| count|\n",
      "+--------------------+------+\n",
      "|                   0|611718|\n",
      "|                   2|610717|\n",
      "|                   4|612004|\n",
      "|                   6|612426|\n",
      "|                   1|611873|\n",
      "|                   3|610666|\n",
      "|                   5|613059|\n",
      "|                   7|611618|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:>                                                          (0 + 0) / 1]\r"
     ]
    }
   ],
   "source": [
    "df.groupBy(F.spark_partition_id()).count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- n_citation: integer (nullable = true)\n",
      " |-- _title_cleaned: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import udf\n",
    "# from pyspark.sql.types import ArrayType, StringType\n",
    "\n",
    "# # Define a very slow tokenizer function\n",
    "# def very_slow_tokenizer(text):\n",
    "#     tokens = []\n",
    "#     for word in text.split():\n",
    "#         for letter in word:\n",
    "#             for i in range(10):\n",
    "#                 tokens.append(letter)\n",
    "#     return tokens\n",
    "\n",
    "# # Define a UDF using the very slow tokenizer function\n",
    "# slow_tokenizer_udf = udf(very_slow_tokenizer, ArrayType(StringType()))\n",
    "\n",
    "# # Apply the UDF to the DataFrame column\n",
    "# df = df.withColumn(\"tokens\", slow_tokenizer_udf(df[\"_c1\"]))\n",
    "# df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------+\n",
      "|title                                                                                                              |token_list                                                                                                                       |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------+\n",
      "|OntologyMediated Queries Combined Complexity and Succinctness of Rewritings via Circuit Complexity                 |[ontologymediated, queries, combined, complexity, and, succinctness, of, rewritings, via, circuit, complexity]                   |\n",
      "|Inducing strong convergence into the asymptotic behaviour of proximal splitting algorithms in Hilbert spaces       |[inducing, strong, convergence, into, the, asymptotic, behaviour, of, proximal, splitting, algorithms, in, hilbert, spaces]      |\n",
      "|SelfSupervised Deep Reinforcement Learning with Generalized Computation Graphs for Robot Navigation                |[selfsupervised, deep, reinforcement, learning, with, generalized, computation, graphs, for, robot, navigation]                  |\n",
      "|Contextdependent upperconfidence bounds for directed exploration                                                   |[contextdependent, upperconfidence, bounds, for, directed, exploration]                                                          |\n",
      "|Unsupervised objectlevel video summarization with online motion autoencoder                                        |[unsupervised, objectlevel, video, summarization, with, online, motion, autoencoder]                                             |\n",
      "|Evaluation of the finite element lattice Boltzmann method for binary fluid flows                                   |[evaluation, of, the, finite, element, lattice, boltzmann, method, for, binary, fluid, flows]                                    |\n",
      "|Universality and individuality in neural dynamics across large populations of recurrent networks                   |[universality, and, individuality, in, neural, dynamics, across, large, populations, of, recurrent, networks]                    |\n",
      "|How to Split ULDL Antennas in FullDuplex Cellular Networks                                                         |[how, to, split, uldl, antennas, in, fullduplex, cellular, networks]                                                             |\n",
      "|Stable Recurrent Models                                                                                            |[stable, recurrent, models]                                                                                                      |\n",
      "|Products of Borel fixed ideals of maximal minors                                                                   |[products, of, borel, fixed, ideals, of, maximal, minors]                                                                        |\n",
      "|Finegrained categorization via CNNbased automatic extraction and integration of objectlevel and partlevel features☆|[finegrained, categorization, via, cnnbased, automatic, extraction, and, integration, of, objectlevel, and, partlevel, features☆]|\n",
      "|Tractability properties of the weighted star discrepancy of the Halton sequence                                    |[tractability, properties, of, the, weighted, star, discrepancy, of, the, halton, sequence]                                      |\n",
      "|SubLinear Time Support Recovery for Compressed Sensing Using SparseGraph Codes                                     |[sublinear, time, support, recovery, for, compressed, sensing, using, sparsegraph, codes]                                        |\n",
      "|A Poisson Gamma Probabilistic Model for Latent Nodegroup Memberships in Dynamic Networks                           |[a, poisson, gamma, probabilistic, model, for, latent, nodegroup, memberships, in, dynamic, networks]                            |\n",
      "|Neuroevolutionbased Inverse Reinforcement Learning                                                                 |[neuroevolutionbased, inverse, reinforcement, learning]                                                                          |\n",
      "|Optimal Approximate Matrix Product in Terms of Stable Rank                                                         |[optimal, approximate, matrix, product, in, terms, of, stable, rank]                                                             |\n",
      "|On the dual representation of coherent risk measures                                                               |[on, the, dual, representation, of, coherent, risk, measures]                                                                    |\n",
      "|Resource Allocation for a Wireless Coexistence Management System based on Reinforcement Learning                   |[resource, allocation, for, a, wireless, coexistence, management, system, based, on, reinforcement, learning]                    |\n",
      "|The Eddy CurrentLLG Equations FEMBEM Coupling and A Priori Error Estimates                                         |[the, eddy, currentllg, equations, fembem, coupling, and, a, priori, error, estimates]                                           |\n",
      "|Contents for a ModelBased Software Engineering Body of Knowledge                                                   |[contents, for, a, modelbased, software, engineering, body, of, knowledge]                                                       |\n",
      "+-------------------------------------------------------------------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Tokenize the titles\n",
    "\n",
    "# Define a user-defined function (UDF) to tokenize\n",
    "tokenizer = udf(lambda x: x.lower().split(), ArrayType(StringType()))\n",
    "\n",
    "# Apply the tokenizer UDF to the \"title\" column of the DataFrame to create a new column called \"token_list\"\n",
    "df = df.withColumn(\"token_list\", tokenizer(col(\"title\").cast(StringType()))) # add this line to cast tokens to StringType\n",
    "df.select(\"title\", \"token_list\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- title: string (nullable = true)\n",
      " |-- n_citation: integer (nullable = true)\n",
      " |-- _title_cleaned: string (nullable = true)\n",
      " |-- token_list: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 15:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|filtered_token                                                                                               |\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "|[matrix, profile, xvi, efficient, effective, labeling, massive, time, series, archives]                      |\n",
      "|[enhancing, cs, with, mobile, apps]                                                                          |\n",
      "|[mobile, crowdsourcing, pervasive, computing, smart, cities]                                                 |\n",
      "|[readability, computer, documentation]                                                                       |\n",
      "|[space, as, interface, creating, interactive, street, art]                                                   |\n",
      "|[comparison, several, difference, schemes, d, d, test, problems, euler, equations]                           |\n",
      "|[foot, trajectory, planning, method, with, adjustable, parameters, complex, environment]                     |\n",
      "|[evolutionary, optimization, sliding, contact, positions, powered, floor, systems, mobile, robots]           |\n",
      "|[multiscale, vortical, structure, analysis, large, eddy, simulation, dune, wake, flow]                       |\n",
      "|[gaussian, kernelbased, fuzzy, cmeans, algorithm, with, spatial, bias, correction]                           |\n",
      "|[towards, building, universal, defect, prediction, model, with, rank, transformed, predictors]               |\n",
      "|[freedom, choice, ease, use, formation, interface, preferences]                                              |\n",
      "|[high, order, stillwater, movingwater, equilibria, preserving, discontinuous, galerkin, methods, ripa, model]|\n",
      "|[surfaces, contours]                                                                                         |\n",
      "|[program, synthesis, opportunities, next, decade]                                                            |\n",
      "|[symbolbased, intelligent, control, system, with, selfexploration, process]                                  |\n",
      "|[hybrid, intelligent, approach, realtime, traffic, control]                                                  |\n",
      "|[discussion, user, experience, panoramic, scooter, riding, video, service]                                   |\n",
      "|[ultrahigh, dimensional, feature, selection, beyond, linear, model]                                          |\n",
      "|[situ, wireless, channel, visualization, using, augmented, reality, ray, tracing]                            |\n",
      "+-------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Define the stop words that should be removed in titles\n",
    "stop_words = [\"the\", \"a\", \"an\", \"and\", \"or\", \"in\", \"on\", \"at\",\\\n",
    "              \"what\",\"why\",\"how\",\"when\",\"whatever\",\"have\",\"has\",\\\n",
    "                  \"to\", \"of\", \"by\", \"for\", \"from\", \"do\", \"how\", \"about\", \"do\", \"does\"]\n",
    "\n",
    "# Remove stop words from token_list and finish cleaning\n",
    "remover = StopWordsRemover(inputCol=\"token_list\", outputCol=\"filtered_token\", stopWords=stop_words)\n",
    "df = remover.transform(df)\n",
    "df.select(\"filtered_token\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Convert tokens to TF-IDF representation\n",
    "hashingTF = HashingTF(inputCol=\"filtered_token\", outputCol=\"rawFeatures\", numFeatures=10000)\n",
    "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"idf_features\")\n",
    "tfidf = idf.fit(hashingTF.transform(df)).transform(hashingTF.transform(df))\n",
    "\n",
    "# pca = PCA(k=2, inputCol=\"idf_features\")\n",
    "# pca.setOutputCol(\"pca_features\")\n",
    "\n",
    "# model = pca.fit(tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Kmeans accuracy to select best k for Kmeans\n",
    "from pyspark.ml.evaluation import ClusteringEvaluator\n",
    "for x in range(10,1000,10):\n",
    "    kmeans = KMeans(featuresCol=\"idf_features\", k=x)\n",
    "    model = kmeans.fit(tfidf)\n",
    "    clustered = model.transform(tfidf)\n",
    "    evaluator = ClusteringEvaluator(predictionCol=\"prediction\", featuresCol=\"idf_features\")\n",
    "    silhouette = evaluator.evaluate(clustered)\n",
    "    print(\"Silhouette =\" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 16:37:19,749 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.JNIBLAS\n",
      "2023-05-05 16:37:19,760 WARN netlib.InstanceBuilder$NativeBLAS: Failed to load implementation from:dev.ludovic.netlib.blas.ForeignLinkerBLAS\n",
      "2023-05-05 16:40:27,700 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1797.8 KiB\n",
      "2023-05-05 16:42:00,972 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1783.9 KiB\n",
      "2023-05-05 16:42:20,689 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1793.0 KiB\n",
      "[Stage 125:>                                                        (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------+----------+\n",
      "|title                                                                                                        |prediction|\n",
      "+-------------------------------------------------------------------------------------------------------------+----------+\n",
      "|Matrix Profile XVI Efficient and Effective Labeling of Massive Time Series Archives                          |0         |\n",
      "|Enhancing CS with Mobile Apps                                                                                |0         |\n",
      "|Mobile Crowdsourcing and Pervasive Computing for Smart Cities                                                |0         |\n",
      "|Readability and computer documentation                                                                       |0         |\n",
      "|Space as interface creating interactive street art                                                           |10        |\n",
      "|Comparison of Several Difference Schemes on D and D Test Problems for the Euler Equations                    |0         |\n",
      "|Foot Trajectory Planning Method with Adjustable Parameters for Complex Environment                           |0         |\n",
      "|Evolutionary optimization of sliding contact positions in powered floor systems for mobile robots            |0         |\n",
      "|Multiscale vortical structure analysis on large eddy simulation of dune wake flow                            |0         |\n",
      "|A Gaussian kernelbased fuzzy cmeans algorithm with a spatial bias correction                                 |0         |\n",
      "|Towards building a universal defect prediction model with rank transformed predictors                        |0         |\n",
      "|Freedom of choice ease of use and the formation of interface preferences                                     |10        |\n",
      "|High Order StillWater and MovingWater Equilibria Preserving Discontinuous Galerkin Methods for the Ripa Model|0         |\n",
      "|Surfaces from contours                                                                                       |0         |\n",
      "|Program synthesis opportunities for the next decade                                                          |0         |\n",
      "|A symbolbased intelligent control system with selfexploration process                                        |0         |\n",
      "|An hybrid intelligent approach for realtime traffic control                                                  |0         |\n",
      "|A Discussion of User Experience on a Panoramic Scooter Riding Video Service                                  |10        |\n",
      "|Ultrahigh Dimensional Feature Selection Beyond The Linear Model                                              |0         |\n",
      "|In Situ Wireless Channel Visualization Using Augmented Reality and Ray Tracing                               |19        |\n",
      "+-------------------------------------------------------------------------------------------------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Cluster the rows using kmeans\n",
    "kmeans = KMeans(featuresCol=\"idf_features\", k=20)\n",
    "model = kmeans.fit(tfidf)\n",
    "clustered = model.transform(tfidf)\n",
    "\n",
    "# Show the clustered dataframe\n",
    "clustered.select(\"title\",\"prediction\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Bisecting K-means accuracy to select best k for Bisecting K-means\n",
    "for x in range(10,1000,10):\n",
    "    bkmeans = BisectingKMeans(featuresCol=\"idf_features\", k=x)\n",
    "    model = bkmeans.fit(tfidf)\n",
    "    clustered = model.transform(tfidf)\n",
    "    evaluator = ClusteringEvaluator(predictionCol=\"prediction\", featuresCol=\"idf_features\")\n",
    "    silhouette = evaluator.evaluate(clustered)\n",
    "    print(\"Silhouette =\" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster the rows using BisectingKMeans\n",
    "\n",
    "# Define the Bisecting K-means model\n",
    "bkmeans = BisectingKMeans(featuresCol=\"idf_features\", k=20, maxIter=20)\n",
    "\n",
    "# Fit the model on the TF-IDF DataFrame\n",
    "model = bkmeans.fit(tfidf)\n",
    "\n",
    "# Get the cluster assignments for each document\n",
    "clustered = model.transform(tfidf)\n",
    "\n",
    "# Show the clustered DataFrame\n",
    "clustered.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 15:38:35,922 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1800.7 KiB\n",
      "2023-05-05 15:40:10,068 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1786.6 KiB\n",
      "[Stage 125:>                                                        (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------------+\n",
      "|prediction|total_citations|percentage_citations|\n",
      "+----------+---------------+--------------------+\n",
      "|7         |76568707       |93.12562671886849   |\n",
      "|13        |953117         |1.1592153170538426  |\n",
      "|0         |769492         |0.9358839604690669  |\n",
      "|1         |739683         |0.8996291781222425  |\n",
      "|6         |486606         |0.5918277908906274  |\n",
      "|17        |446128         |0.5425969854347333  |\n",
      "|4         |425741         |0.5178015797617922  |\n",
      "|11        |394807         |0.4801785317857897  |\n",
      "|3         |298491         |0.36303553415028644 |\n",
      "|18        |239454         |0.2912326026393516  |\n",
      "|12        |222755         |0.2709226757578857  |\n",
      "|2         |171856         |0.20901747374939822 |\n",
      "|15        |160194         |0.19483372817830683 |\n",
      "|14        |109541         |0.13322772025406637 |\n",
      "|5         |109478         |0.13315109737883238 |\n",
      "|8         |42403          |0.05157205997693262 |\n",
      "|10        |35200          |0.04281151124184676 |\n",
      "|16        |21464          |0.026105291968607926|\n",
      "|9         |19683          |0.023939175448104255|\n",
      "|19        |6077           |0.007391066869792692|\n",
      "+----------+---------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#inefficien version of calculating the percentage of being influentioal\n",
    "# Group by the cluster and sum the number of citations in each cluster\n",
    "citations = clustered.groupBy(\"prediction\").agg(sum(\"n_citation\").alias(\"total_citations\"))\n",
    "\n",
    "# Collect all data to the driver node\n",
    "CitationList = citations.collect()\n",
    "\n",
    "# Calculate the total number of citations\n",
    "total_citations = 0\n",
    "for row in CitationList:\n",
    "    total_citations += row[\"total_citations\"]\n",
    "\n",
    "# Calculate the percentage of citations in each cluster\n",
    "citations = spark.createDataFrame(CitationList)\n",
    "citations = citations.withColumn(\"percentage_citations\", col(\"total_citations\")/total_citations*100)\n",
    "\n",
    "# Sort the result by the percentage of citations in descending order\n",
    "citations = citations.sort(col(\"percentage_citations\").desc())\n",
    "\n",
    "# Show the result\n",
    "citations.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 15:48:22,632 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1833.3 KiB\n",
      "2023-05-05 15:48:33,102 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1836.9 KiB\n",
      "2023-05-05 15:48:33,491 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1833.4 KiB\n",
      "[Stage 150:====================================================>(198 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------------+\n",
      "|prediction|total_citations|percentage_citations|\n",
      "+----------+---------------+--------------------+\n",
      "|12        |222755         |0.2709226757578857  |\n",
      "|1         |739683         |0.8996291781222425  |\n",
      "|13        |953117         |1.1592153170538426  |\n",
      "|6         |486606         |0.5918277908906274  |\n",
      "|16        |21464          |0.026105291968607926|\n",
      "|3         |298491         |0.36303553415028644 |\n",
      "|5         |109478         |0.13315109737883238 |\n",
      "|19        |6077           |0.007391066869792692|\n",
      "|15        |160194         |0.19483372817830683 |\n",
      "|17        |446128         |0.5425969854347333  |\n",
      "|9         |19683          |0.023939175448104255|\n",
      "|4         |425741         |0.5178015797617922  |\n",
      "|8         |42403          |0.05157205997693262 |\n",
      "|7         |76568707       |93.12562671886849   |\n",
      "|10        |35200          |0.04281151124184676 |\n",
      "|11        |394807         |0.4801785317857897  |\n",
      "|14        |109541         |0.13322772025406637 |\n",
      "|2         |171856         |0.20901747374939822 |\n",
      "|0         |769492         |0.9358839604690669  |\n",
      "|18        |239454         |0.2912326026393516  |\n",
      "+----------+---------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 15:48:42,855 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1834.3 KiB\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#Efficient version of calulatating percentage of being influential\n",
    "\n",
    "# Group by the cluster and sum the number of citations in each cluster\n",
    "citations = clustered.groupBy(\"prediction\").agg(sum(\"n_citation\").alias(\"total_citations\"))\n",
    "# Calculate the total number of citations\n",
    "total_citations = citations.agg(sum(\"total_citations\")).collect()[0][0]\n",
    "# Calculate the percentage of citations in each cluster\n",
    "citations = citations.withColumn(\"percentage_citations\", col(\"total_citations\")/total_citations*100)\n",
    "\n",
    "# Show the result\n",
    "citations.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Efficient version of calulatating percentage of being influential\n",
    "# Apply Caching technique\n",
    "# Group by the cluster and sum the number of citations in each cluster\n",
    "citations = clustered.groupBy(\"prediction\").agg(sum(\"n_citation\").alias(\"total_citations\"))\n",
    "clustered.cache()\n",
    "# Calculate the total number of citations\n",
    "total_citations = citations.agg(sum(\"total_citations\")).collect()[0][0]\n",
    "clustered.cache()\n",
    "# Calculate the percentage of citations in each cluster\n",
    "citations = citations.withColumn(\"percentage_citations\", col(\"total_citations\")/total_citations*100)\n",
    "\n",
    "# Show the result\n",
    "citations.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 15:51:08,758 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1833.3 KiB\n",
      "2023-05-05 15:51:18,958 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1836.9 KiB\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time of groupBy function is: 0.02834033966064453\n",
      "Time of agg function is: 0.027842283248901367\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-05 15:51:19,266 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1833.4 KiB\n",
      "2023-05-05 15:51:28,338 WARN scheduler.DAGScheduler: Broadcasting large task binary with size 1834.3 KiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+--------------------+\n",
      "|prediction|total_citations|percentage_citations|\n",
      "+----------+---------------+--------------------+\n",
      "|12        |222755         |0.2709226757578857  |\n",
      "|1         |739683         |0.8996291781222425  |\n",
      "|13        |953117         |1.1592153170538426  |\n",
      "|6         |486606         |0.5918277908906274  |\n",
      "|16        |21464          |0.026105291968607926|\n",
      "|3         |298491         |0.36303553415028644 |\n",
      "|5         |109478         |0.13315109737883238 |\n",
      "|19        |6077           |0.007391066869792692|\n",
      "|15        |160194         |0.19483372817830683 |\n",
      "|17        |446128         |0.5425969854347333  |\n",
      "|9         |19683          |0.023939175448104255|\n",
      "|4         |425741         |0.5178015797617922  |\n",
      "|8         |42403          |0.05157205997693262 |\n",
      "|7         |76568707       |93.12562671886849   |\n",
      "|10        |35200          |0.04281151124184676 |\n",
      "|11        |394807         |0.4801785317857897  |\n",
      "|14        |109541         |0.13322772025406637 |\n",
      "|2         |171856         |0.20901747374939822 |\n",
      "|0         |769492         |0.9358839604690669  |\n",
      "|18        |239454         |0.2912326026393516  |\n",
      "+----------+---------------+--------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Benchmarking to figure out functions that are not optimized by calculating Execution time\n",
    "# Time of groupBy() operation\n",
    "start_time = time.time()\n",
    "citations_groupBy = clustered.groupBy(\"prediction\")\n",
    "end_time = time.time()\n",
    "time_of_groupBy = end_time - start_time\n",
    "\n",
    "# Time of agg operation\n",
    "start_time = time.time()\n",
    "citations_aggregated = citations_groupBy.agg(sum(\"n_citation\").alias(\"total_citations\"))\n",
    "end_time = time.time()\n",
    "time_of_agg = end_time - start_time\n",
    "\n",
    "# Calculate the total number of citations\n",
    "total_citations = citations_aggregated.agg(sum(\"total_citations\")).collect()[0][0]\n",
    "\n",
    "# Calculate to find the percentage of citations for each cluster\n",
    "Percentage_of_citation = citations_aggregated.withColumn(\"percentage_citations\", col(\"total_citations\")/total_citations*100)\n",
    "\n",
    "print(\"Time of groupBy function is:\", time_of_groupBy)\n",
    "\n",
    "print(\"Time of agg function is:\", time_of_agg)\n",
    "\n",
    "# Show the result\n",
    "Percentage_of_citation.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Delta table in an output directory on HDFS\n",
    "citations.write.mode(\"overwrite\").option(\"delimiter\", \"\\t\").csv(\"hdfs://namenode:9000/project/output\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
